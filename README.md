# Worldmodel2025_final
Used to create the final assignment for the World Model 2025.


## ベースラインモデルについて
Google Researchが発表した「動画から物体を個別に切り出し、時間的に追跡・予測する」というタスクのデファクトスタンダードとなっているSlot Attention for Video(以下略SAVi)を再現実装する

### なぜこのモデルが本研究においてベースラインとして最適か

*   入力が動画でピクセルであるという点
*   物体ごとに分離して学習しているという点
*   物体は見た目に依存しているという点
*   時間経過でトラッキングが起きやすいという点
*   複雑なテクスチャの実写映像に弱いという点

### 本研究で言いたいこと

物体の見た目だけを捉え意味を捉えていないSAViと物体の意味を捉えた本研究を比べ、意味を捉えていると物理的一貫性が確立されやすく、覚えることが少ないため、メモリ効率が良いということを示したい。

### 使用するデータセット
意味（appearance）と位置（pose / motion）を分離できているか」を、
動画レベルで厳密に評価できる**MOVi**を使用する。

MOVi は以下を全て満たします。

| 要件                         | MOVi                          |
| -------------------------- | ----------------------------- |
| 動画データ                      | ✅                             |
| 複数オブジェクト                   | ✅                             |
| オブジェクト中心表現                 | ✅（instance segmentation GTあり） |
| 位置変化（運動）                   | ✅                             |
| 視点変化                       | ✅                             |
| 遮蔽（occlusion）              | ✅                             |
| 長期依存（temporal consistency） | ✅                             |
| 定量評価しやすい                   | ✅（mask / tracking / depth）    |

まず再現性を一旦確保したいので**MOVi-A**を使用する。今後MOVi-D、MOVi-Eを使用していきたい。また動画長は30にする。


