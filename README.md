# Worldmodel2025_final
## タイトル
動画生成のための物体中心世界モデル：意味・位置分離による効率的な長期記憶メカニズムの提案

## 研究概要
近年の動画生成AIにおいて、生成時間が長くなるにつれて被写体の特徴や物理的な一貫性が失われる「長期記憶の欠如」は重大な課題である。従来の世界モデルは、背景ノイズを含む画素レベルの情報を過剰に保持しようとするため、メモリ効率が悪く、長期的な文脈維持が困難であった。
そこで本研究では、この課題を解決するため、意味的抽象化と空間構造の分離に基づく新たな物体中心世界モデルを提案する。具体的には、映像内の各物体をVLM（Video Language Model）やマルチモーダル統合技術を用いて「視覚・聴覚・質感」を集約した不変の意味ベクトルとして抽象化し、その空間的な位置関係（座標・構造）を可変情報として分離して保存する。
この「意味と位置の分離（Disentanglement）」により、記憶容量を劇的に削減しつつ、ピクセルレベルの微細な変化に左右されない頑健な記憶メカニズムを実現する。本手法は、長時間のシーケンスにおいても物体のアイデンティティを保持し続け、一貫性のある高品質な長尺動画生成への貢献を目指すものである。



## ベースラインモデルについて
Google Researchが発表した「動画から物体を個別に切り出し、時間的に追跡・予測する」というタスクのデファクトスタンダードとなっているSlot Attention for Video(以下略SAVi)を再現実装する

### なぜこのモデルが本研究においてベースラインとして最適か

*   入力が動画でピクセルであるという点
*   物体ごとに分離して学習しているという点
*   物体は見た目に依存しているという点
*   時間経過でトラッキングが起きやすいという点
*   複雑なテクスチャの実写映像に弱いという点

### 本研究で言いたいこと

物体の見た目だけを捉え意味を捉えていないSAViと物体の意味を捉えた本研究を比べ、意味を捉えていると物理的一貫性が確立されやすく、覚えることが少ないため、メモリ効率が良いということを示したい。

### 使用するデータセット
意味（appearance）と位置（pose / motion）を分離できているか」を、
動画レベルで厳密に評価できる**MOVi**を使用する。

MOVi は以下を全て満たします。

| 要件                         | MOVi                          |
| -------------------------- | ----------------------------- |
| 動画データ                      | ✅                             |
| 複数オブジェクト                   | ✅                             |
| オブジェクト中心表現                 | ✅（instance segmentation GTあり） |
| 位置変化（運動）                   | ✅                             |
| 視点変化                       | ✅                             |
| 遮蔽（occlusion）              | ✅                             |
| 長期依存（temporal consistency） | ✅                             |
| 定量評価しやすい                   | ✅（mask / tracking / depth）    |

まず再現性を一旦確保したいので**MOVi-A**を使用する。今後MOVi-D、MOVi-Eを使用していきたい。また動画長は30にする。


