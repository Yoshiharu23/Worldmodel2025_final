{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Slot Attention × MOVi-A（フレーム抽出）再現ベースライン\n",
        "\n",
        "保存するもの（比較のため必須）：\n",
        "- results/<RUN_NAME>/<timestamp>/\n",
        "  - config.json\n",
        "  - metrics.csv              (step, train_loss, lr, val_loss, ari(optional))\n",
        "  - loss_curve.png           (損失曲線)\n",
        "  - recon_stepXXXXXX.png     (固定バッチ: input/recon/slot分解)\n"
      ],
      "metadata": {
        "id": "3-E5ZQT54THf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## インストール"
      ],
      "metadata": {
        "id": "NQgKBSMKRkYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U tensorflow-datasets tensorflow-io-gcs-filesystem einops matplotlib tqdm\n",
        "!pip -q install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
        "print(\"done\")"
      ],
      "metadata": {
        "id": "wqbneb8i3xkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## import・乱数固定・GPU確認"
      ],
      "metadata": {
        "id": "PDGMRZFaRiuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time, json, math, random\n",
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import IterableDataset, DataLoader\n",
        "\n",
        "from einops import rearrange\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def seed_all(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "seed_all(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device:\", device, \"cuda:\", torch.cuda.is_available())\n",
        "print(\"torch:\", torch.__version__)\n",
        "print(\"tfds:\", tfds.__version__)"
      ],
      "metadata": {
        "id": "EJYN-diZ3xrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MOVi-A をロード"
      ],
      "metadata": {
        "id": "Be6aZ819RsRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_NAME = \"kubric:movi_a\"\n",
        "\n",
        "# 公開GCSの生成済みTFDSを複数バージョン候補で探索\n",
        "GCS_BASE = \"gs://kubric-public/tfds/movi_a\"\n",
        "GCS_VERSION_CANDIDATES = [\"1.0.0\", \"1.0.1\", \"1.0.2\", \"1.1.0\", \"1.2.0\"]\n",
        "\n",
        "def try_builder_from_gcs():\n",
        "    for ver in GCS_VERSION_CANDIDATES:\n",
        "        gcs_dir = f\"{GCS_BASE}/{ver}\"\n",
        "        try:\n",
        "            b = tfds.builder_from_directory(gcs_dir)\n",
        "            print(f\"[OK] builder_from_directory: {gcs_dir}\")\n",
        "            return b\n",
        "        except Exception as e:\n",
        "            print(f\"[NG] {gcs_dir} -> {type(e).__name__}: {e}\")\n",
        "    return None\n",
        "\n",
        "builder = try_builder_from_gcs()\n",
        "\n",
        "if builder is None:\n",
        "    print(\"\\nGCSから読めないため、ローカルで download_and_prepare() します（初回は重いです）\")\n",
        "    builder = tfds.builder(DATASET_NAME)\n",
        "    builder.download_and_prepare()\n",
        "\n",
        "info = builder.info\n",
        "print(\"\\n=== Dataset Info ===\")\n",
        "print(\"splits:\", list(info.splits.keys()))\n",
        "print(\"features:\", info.features)"
      ],
      "metadata": {
        "id": "VMVYf1BD3xyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train/val を作成 & キー確認（video_key / mask_key自動推定）"
      ],
      "metadata": {
        "id": "tAErZx02RwkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_tfds = builder.as_dataset(split=\"train\", shuffle_files=True)\n",
        "val_split = \"validation\" if \"validation\" in builder.info.splits else (\"val\" if \"val\" in builder.info.splits else None)\n",
        "val_tfds = builder.as_dataset(split=val_split, shuffle_files=False) if val_split else None\n",
        "\n",
        "ex = next(iter(tfds.as_numpy(train_tfds.take(1))))\n",
        "print(\"keys:\", list(ex.keys()))\n",
        "for k, v in ex.items():\n",
        "    if hasattr(v, \"shape\"):\n",
        "        print(f\"{k}: shape={v.shape}, dtype={v.dtype}\")\n",
        "    else:\n",
        "        print(f\"{k}: type={type(v)}\")\n",
        "\n",
        "# video key\n",
        "video_key = None\n",
        "for k in [\"video\", \"frames\", \"images\", \"image\"]:\n",
        "    if k in ex:\n",
        "        video_key = k\n",
        "        break\n",
        "assert video_key is not None, \"動画キーが見つかりません\"\n",
        "print(\"video_key =\", video_key)\n",
        "\n",
        "# mask key (optional, ARI用)\n",
        "mask_key = None\n",
        "for k in [\"segmentations\", \"segmentation\", \"instance_segmentation\", \"instances\", \"masks\"]:\n",
        "    if k in ex:\n",
        "        mask_key = k\n",
        "        break\n",
        "print(\"mask_key =\", mask_key)"
      ],
      "metadata": {
        "id": "vUHWCwYW3x4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TFDS → PyTorch（フレーム抽出 DataLoader）"
      ],
      "metadata": {
        "id": "ijroySx5R1ZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== 修正セル1（最終版）：TFDS -> ローカルキャッシュ -> PyTorch DataLoader（堅牢版） ======\n",
        "import os, time, math, json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# -----------------------\n",
        "# 0) 必須変数の補完（未定義なら安全なデフォルトを置く）\n",
        "# -----------------------\n",
        "if \"IMG_SIZE\" not in globals():\n",
        "    IMG_SIZE = 128\n",
        "if \"BATCH_SIZE\" not in globals():\n",
        "    BATCH_SIZE = 32\n",
        "if \"RANDOM_FRAME\" not in globals():\n",
        "    RANDOM_FRAME = True\n",
        "if \"device\" not in globals():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if \"OUT_DIR\" not in globals():\n",
        "    OUT_DIR = \"/content/results/tmp_run\"\n",
        "    os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"IMG_SIZE:\", IMG_SIZE, \"BATCH_SIZE:\", BATCH_SIZE, \"RANDOM_FRAME:\", RANDOM_FRAME)\n",
        "print(\"device:\", device)\n",
        "print(\"OUT_DIR:\", OUT_DIR)\n",
        "\n",
        "# -----------------------\n",
        "# 1) train_tfds / val_tfds が無ければ、ここでMOVi-Aを作る（順番実行でも落ちない）\n",
        "# -----------------------\n",
        "if \"train_tfds\" not in globals():\n",
        "    print(\"[info] train_tfds not found -> build MOVi-A dataset now\")\n",
        "    DATASET_NAME = \"kubric:movi_a\"\n",
        "\n",
        "    # 可能なら公開GCSを使う（速いことがある）\n",
        "    GCS_BASE = \"gs://kubric-public/tfds/movi_a\"\n",
        "    GCS_VERSION_CANDIDATES = [\"1.2.0\",\"1.1.0\",\"1.0.2\",\"1.0.1\",\"1.0.0\"]\n",
        "\n",
        "    builder = None\n",
        "    for ver in GCS_VERSION_CANDIDATES:\n",
        "        gcs_dir = f\"{GCS_BASE}/{ver}\"\n",
        "        try:\n",
        "            builder = tfds.builder_from_directory(gcs_dir)\n",
        "            print(f\"[OK] builder_from_directory: {gcs_dir}\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"[NG] {gcs_dir} -> {type(e).__name__}: {e}\")\n",
        "\n",
        "    if builder is None:\n",
        "        print(\"[fallback] download_and_prepare() (may take time on first run)\")\n",
        "        builder = tfds.builder(DATASET_NAME)\n",
        "        builder.download_and_prepare()\n",
        "\n",
        "    train_tfds = builder.as_dataset(split=\"train\", shuffle_files=True)\n",
        "    val_split = \"validation\" if \"validation\" in builder.info.splits else (\"val\" if \"val\" in builder.info.splits else None)\n",
        "    val_tfds = builder.as_dataset(split=val_split, shuffle_files=False) if val_split else None\n",
        "    print(\"train_tfds / val_tfds ready\")\n",
        "\n",
        "# -----------------------\n",
        "# 2) video_key が無ければ推定（順番実行でも落ちない）\n",
        "# -----------------------\n",
        "if \"video_key\" not in globals():\n",
        "    ex = next(iter(tfds.as_numpy(train_tfds.take(1))))\n",
        "    keys = list(ex.keys())\n",
        "    print(\"[info] infer video_key from keys:\", keys)\n",
        "    video_key = None\n",
        "    for k in [\"video\", \"frames\", \"images\", \"image\"]:\n",
        "        if k in ex:\n",
        "            video_key = k\n",
        "            break\n",
        "    assert video_key is not None, f\"video_key not found. keys={keys}\"\n",
        "    print(\"[OK] video_key =\", video_key)\n",
        "\n",
        "# -----------------------\n",
        "# 3) キャッシュ作成（OOM対策：CACHE_Nを自動で縮める）\n",
        "#    - CPUメモリ節約のため float16 で保存\n",
        "# -----------------------\n",
        "CACHE_N_REQUEST = 20000  # まずは希望値\n",
        "CACHE_N_MIN = 2000       # これ未満にはしない\n",
        "CACHE_DTYPE = torch.float16\n",
        "\n",
        "cache_path = f\"/content/moviA_cache_{IMG_SIZE}_{CACHE_N_REQUEST}.pt\"\n",
        "\n",
        "def build_cache(cache_n):\n",
        "    print(f\"[cache] building N={cache_n} dtype={CACHE_DTYPE} ...\")\n",
        "    t0 = time.time()\n",
        "\n",
        "    # 先にテンソル確保（OOMならここで落ちるので縮めて再試行しやすい）\n",
        "    cache = torch.empty((cache_n, 3, IMG_SIZE, IMG_SIZE), dtype=CACHE_DTYPE, device=\"cpu\")\n",
        "\n",
        "    it = tfds.as_numpy(train_tfds.take(cache_n))\n",
        "    for i, ex in enumerate(it):\n",
        "        vid = ex[video_key]  # [T,H,W,3] uint8\n",
        "        Tv = vid.shape[0]\n",
        "        t = np.random.randint(0, Tv) if RANDOM_FRAME else 0\n",
        "        frame = vid[t].astype(np.float32) / 255.0\n",
        "        frame = frame * 2.0 - 1.0  # [-1,1]\n",
        "\n",
        "        x = torch.from_numpy(frame).permute(2,0,1).unsqueeze(0)  # [1,3,H,W]\n",
        "        x = F.interpolate(x, size=(IMG_SIZE, IMG_SIZE), mode=\"bilinear\", align_corners=False)\n",
        "        x = x.squeeze(0).contiguous()  # [3,H,W]\n",
        "\n",
        "        cache[i].copy_(x.to(dtype=CACHE_DTYPE))\n",
        "\n",
        "        if (i+1) % 1000 == 0:\n",
        "            print(f\"  cached {i+1}/{cache_n}  elapsed={time.time()-t0:.1f}s\")\n",
        "\n",
        "    print(f\"[cache] build done sec={time.time()-t0:.1f}s\")\n",
        "    return cache\n",
        "\n",
        "# 既存キャッシュがあればそれを優先\n",
        "cache_x = None\n",
        "if os.path.exists(cache_path):\n",
        "    print(\"[cache] loading existing:\", cache_path)\n",
        "    cache_x = torch.load(cache_path, map_location=\"cpu\")\n",
        "    print(\"[cache] loaded:\", cache_x.shape, cache_x.dtype)\n",
        "else:\n",
        "    # OOMなら自動で半分にして再試行\n",
        "    cache_n = CACHE_N_REQUEST\n",
        "    while True:\n",
        "        try:\n",
        "            cache_x = build_cache(cache_n)\n",
        "            torch.save(cache_x, f\"/content/moviA_cache_{IMG_SIZE}_{cache_n}.pt\")\n",
        "            cache_path = f\"/content/moviA_cache_{IMG_SIZE}_{cache_n}.pt\"\n",
        "            print(\"[cache] saved:\", cache_path)\n",
        "            break\n",
        "        except RuntimeError as e:\n",
        "            msg = str(e).lower()\n",
        "            if (\"out of memory\" in msg) or (\"cannot allocate\" in msg):\n",
        "                new_n = cache_n // 2\n",
        "                if new_n < CACHE_N_MIN:\n",
        "                    raise RuntimeError(f\"OOM even with CACHE_N={cache_n}. Reduce IMG_SIZE or CACHE_N.\") from e\n",
        "                print(f\"[cache] OOM at N={cache_n} -> retry with N={new_n}\")\n",
        "                cache_n = new_n\n",
        "                continue\n",
        "            raise\n",
        "\n",
        "# -----------------------\n",
        "# 4) PyTorch DataLoader（ここから高速）\n",
        "# -----------------------\n",
        "train_ds_cached = TensorDataset(cache_x)  # (x,) が返る\n",
        "train_loader = DataLoader(\n",
        "    train_ds_cached,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2,              # TFDSじゃないので workers OK\n",
        "    pin_memory=True,\n",
        "    drop_last=True,\n",
        "    persistent_workers=True\n",
        ")\n",
        "\n",
        "# valキャッシュ（小さめで十分）\n",
        "val_loader = None\n",
        "if \"val_tfds\" in globals() and (val_tfds is not None):\n",
        "    VAL_CACHE_N = min(2000, cache_x.shape[0] // 10)\n",
        "    val_cache_path = f\"/content/moviA_valcache_{IMG_SIZE}_{VAL_CACHE_N}.pt\"\n",
        "    if os.path.exists(val_cache_path):\n",
        "        val_x = torch.load(val_cache_path, map_location=\"cpu\")\n",
        "    else:\n",
        "        val_x = torch.empty((VAL_CACHE_N, 3, IMG_SIZE, IMG_SIZE), dtype=CACHE_DTYPE, device=\"cpu\")\n",
        "        it = tfds.as_numpy(val_tfds.take(VAL_CACHE_N))\n",
        "        for i, ex in enumerate(it):\n",
        "            vid = ex[video_key]\n",
        "            frame = vid[0].astype(np.float32)/255.0\n",
        "            frame = frame*2.0 - 1.0\n",
        "            x = torch.from_numpy(frame).permute(2,0,1).unsqueeze(0)\n",
        "            x = F.interpolate(x, size=(IMG_SIZE, IMG_SIZE), mode=\"bilinear\", align_corners=False)\n",
        "            val_x[i].copy_(x.squeeze(0).contiguous().to(dtype=CACHE_DTYPE))\n",
        "        torch.save(val_x, val_cache_path)\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        TensorDataset(val_x),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        drop_last=False,\n",
        "        persistent_workers=True\n",
        "    )\n",
        "\n",
        "# -----------------------\n",
        "# 5) 最終チェック（ここが速ければOK）\n",
        "# -----------------------\n",
        "t0 = time.time()\n",
        "(xb,) = next(iter(train_loader))\n",
        "sec = time.time() - t0\n",
        "print(\"next(train_loader) sec:\", sec, \"batch:\", xb.shape, xb.dtype, \"range:\", (xb.min().item(), xb.max().item()))\n",
        "print(\"cache_path:\", cache_path)"
      ],
      "metadata": {
        "id": "bd_vMLjc3x-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 固定評価バッチ（比較用：毎回同じ入力）"
      ],
      "metadata": {
        "id": "YvXuP_V-R6Dd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 固定評価バッチ（fixed_x / fixed_m）作成：堅牢版 =====\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# --- 未定義ならここでデフォルトを置く（順番実行で落ちない） ---\n",
        "if \"EVAL_SEED\" not in globals():\n",
        "    EVAL_SEED = 123\n",
        "if \"EVAL_BATCH_SIZE\" not in globals():\n",
        "    EVAL_BATCH_SIZE = 8\n",
        "# 以前の変数名揺れ対策（EVAL_NUM_SAMPLES が無い場合）\n",
        "if \"EVAL_NUM_SAMPLES\" not in globals():\n",
        "    EVAL_NUM_SAMPLES = 256\n",
        "\n",
        "# 必須：train_tfds / video_key / IMG_SIZE\n",
        "assert \"train_tfds\" in globals(), \"train_tfds が未定義です（データロードセルを先に実行してください）\"\n",
        "assert \"video_key\" in globals(), \"video_key が未定義です（キー推定セルを先に実行してください）\"\n",
        "if \"IMG_SIZE\" not in globals():\n",
        "    IMG_SIZE = 128\n",
        "\n",
        "# mask_key は無くてもOK\n",
        "if \"mask_key\" not in globals():\n",
        "    mask_key = None\n",
        "\n",
        "# TFDSFrameIterable が無い場合のフォールバック（このセル単体でも動く）\n",
        "if \"TFDSFrameIterable\" not in globals():\n",
        "    import tensorflow_datasets as tfds\n",
        "    class TFDSFrameIterable:\n",
        "        def __init__(self, tfds_dataset, video_key, mask_key=None, img_size=128, random_frame=False, seed=0):\n",
        "            self.ds = tfds_dataset\n",
        "            self.video_key = video_key\n",
        "            self.mask_key = mask_key\n",
        "            self.img_size = img_size\n",
        "            self.random_frame = random_frame\n",
        "            self.rng = np.random.RandomState(seed)\n",
        "\n",
        "        def __iter__(self):\n",
        "            for ex in tfds.as_numpy(self.ds):\n",
        "                vid = ex[self.video_key]\n",
        "                T = vid.shape[0]\n",
        "                t = self.rng.randint(0, T) if self.random_frame else 0\n",
        "                frame = vid[t].astype(np.float32)/255.0\n",
        "                frame = frame*2.0 - 1.0  # [-1,1]\n",
        "                x = torch.from_numpy(frame).permute(2,0,1).unsqueeze(0)   # [1,3,H,W]\n",
        "                x = F.interpolate(x, size=(self.img_size,self.img_size), mode=\"bilinear\", align_corners=False)\n",
        "                x = x.squeeze(0).contiguous()\n",
        "\n",
        "                if self.mask_key is not None and self.mask_key in ex:\n",
        "                    m = ex[self.mask_key][t]\n",
        "                    if m.ndim == 3 and m.shape[-1] == 1:\n",
        "                        m = m[...,0]\n",
        "                    m = torch.from_numpy(m.astype(np.int64)).unsqueeze(0).unsqueeze(0)  # [1,1,H,W]\n",
        "                    m = F.interpolate(m.float(), size=(self.img_size,self.img_size), mode=\"nearest\").long()\n",
        "                    m = m.squeeze(0).squeeze(0)  # [H,W]\n",
        "                    yield x, m\n",
        "                else:\n",
        "                    yield x\n",
        "\n",
        "def build_fixed_eval_batch(tfds_ds, n_pool=256, batch_size=8, seed=123):\n",
        "    \"\"\"\n",
        "    tfds_ds: tfds dataset\n",
        "    returns:\n",
        "      fixed_x: [B,3,H,W] float32 [-1,1]\n",
        "      fixed_m: [B,H,W]   int64 (if mask_key exists) else None\n",
        "    \"\"\"\n",
        "    pool = tfds_ds.take(n_pool)\n",
        "    it = TFDSFrameIterable(pool, video_key=video_key, mask_key=mask_key,\n",
        "                           img_size=IMG_SIZE, random_frame=False, seed=seed)\n",
        "\n",
        "    xs, ms = [], []\n",
        "    rng = np.random.RandomState(seed)\n",
        "\n",
        "    for item in it:\n",
        "        if isinstance(item, (list, tuple)):\n",
        "            x, m = item\n",
        "            xs.append(x); ms.append(m)\n",
        "        else:\n",
        "            xs.append(item)\n",
        "        if len(xs) >= n_pool:\n",
        "            break\n",
        "\n",
        "    if len(xs) == 0:\n",
        "        raise RuntimeError(\"fixed eval batch を作れませんでした（tfds_ds.take(n_pool) が空の可能性）\")\n",
        "\n",
        "    idx = np.arange(len(xs))\n",
        "    rng.shuffle(idx)\n",
        "    idx = idx[:batch_size]\n",
        "\n",
        "    fixed_x = torch.stack([xs[i] for i in idx], dim=0).float()\n",
        "    fixed_m = torch.stack([ms[i] for i in idx], dim=0).long() if len(ms) > 0 else None\n",
        "    return fixed_x, fixed_m\n",
        "\n",
        "# 実行（変数名揺れがあっても必ず動く）\n",
        "fixed_x, fixed_m = build_fixed_eval_batch(\n",
        "    train_tfds,\n",
        "    n_pool=EVAL_NUM_SAMPLES,\n",
        "    batch_size=EVAL_BATCH_SIZE,\n",
        "    seed=EVAL_SEED\n",
        ")\n",
        "\n",
        "print(\"fixed_x:\", fixed_x.shape, fixed_x.dtype, \"range:\", (fixed_x.min().item(), fixed_x.max().item()))\n",
        "print(\"fixed_m:\", None if fixed_m is None else (fixed_m.shape, fixed_m.dtype))"
      ],
      "metadata": {
        "id": "52oXEjj33yHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Slot Attention"
      ],
      "metadata": {
        "id": "1PQ8TdngR-I6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SlotAttention(nn.Module):\n",
        "    def __init__(self, num_slots=7, dim=64, iters=3, hidden_dim=128, eps=1e-8):\n",
        "        super().__init__()\n",
        "        self.num_slots = num_slots\n",
        "        self.iters = iters\n",
        "        self.dim = dim\n",
        "        self.eps = eps\n",
        "\n",
        "        self.norm_inputs = nn.LayerNorm(dim)\n",
        "        self.norm_slots  = nn.LayerNorm(dim)\n",
        "        self.norm_mlp    = nn.LayerNorm(dim)\n",
        "\n",
        "        self.to_q = nn.Linear(dim, dim, bias=False)\n",
        "        self.to_k = nn.Linear(dim, dim, bias=False)\n",
        "        self.to_v = nn.Linear(dim, dim, bias=False)\n",
        "\n",
        "        self.gru = nn.GRUCell(dim, dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden_dim, dim)\n",
        "        )\n",
        "\n",
        "        self.slots_mu = nn.Parameter(torch.zeros(1, 1, dim))\n",
        "        self.slots_sigma = nn.Parameter(torch.ones(1, 1, dim))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        B, N, D = inputs.shape\n",
        "        inputs = self.norm_inputs(inputs)\n",
        "\n",
        "        mu = self.slots_mu.expand(B, self.num_slots, -1)\n",
        "        sigma = self.slots_sigma.expand(B, self.num_slots, -1)\n",
        "        slots = mu + sigma * torch.randn_like(mu)\n",
        "\n",
        "        k = self.to_k(inputs)\n",
        "        v = self.to_v(inputs)\n",
        "\n",
        "        for _ in range(self.iters):\n",
        "            slots_prev = slots\n",
        "            slots_norm = self.norm_slots(slots)\n",
        "            q = self.to_q(slots_norm)\n",
        "\n",
        "            attn_logits = torch.einsum(\"bnd,bkd->bnk\", k, q) * (D ** -0.5)\n",
        "            attn = F.softmax(attn_logits, dim=-1) + self.eps  # over slots\n",
        "\n",
        "            attn_norm = attn / attn.sum(dim=1, keepdim=True)\n",
        "            updates = torch.einsum(\"bnk,bnd->bkd\", attn_norm, v)\n",
        "\n",
        "            slots = self.gru(\n",
        "                updates.reshape(B*self.num_slots, D),\n",
        "                slots_prev.reshape(B*self.num_slots, D)\n",
        "            ).reshape(B, self.num_slots, D)\n",
        "\n",
        "            slots = slots + self.mlp(self.norm_mlp(slots))\n",
        "\n",
        "        return slots, attn  # slots [B,K,D], attn [B,N,K]"
      ],
      "metadata": {
        "id": "pmrYgaxm3yNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder / Decoder / AutoEncoder"
      ],
      "metadata": {
        "id": "oPOaK05_SBPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftPositionEmbed(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(4, hidden_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        device = x.device\n",
        "        ys = torch.linspace(0., 1., H, device=device)\n",
        "        xs = torch.linspace(0., 1., W, device=device)\n",
        "        yy, xx = torch.meshgrid(ys, xs, indexing=\"ij\")\n",
        "        coords = torch.stack([xx, yy, 1.-xx, 1.-yy], dim=-1)  # [H,W,4]\n",
        "        emb = self.dense(coords).permute(2,0,1).unsqueeze(0)  # [1,C,H,W]\n",
        "        return x + emb\n",
        "\n",
        "class CNNEncoder(nn.Module):\n",
        "    def __init__(self, in_ch=3, hidden=64):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, hidden, 5, padding=2), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(hidden, hidden, 5, padding=2), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(hidden, hidden, 5, padding=2), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(hidden, hidden, 5, padding=2), nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.pos = SoftPositionEmbed(hidden)\n",
        "        self.ln = nn.LayerNorm(hidden)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(hidden, hidden), nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden, hidden),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.conv(x)\n",
        "        h = self.pos(h)\n",
        "        B, C, H, W = h.shape\n",
        "        h = h.permute(0,2,3,1).reshape(B, H*W, C)\n",
        "        h = self.ln(h)\n",
        "        h = self.mlp(h)\n",
        "        return h, (H, W)\n",
        "\n",
        "class SpatialBroadcastDecoder(nn.Module):\n",
        "    def __init__(self, slot_dim=64, hidden=64, out_size=128, init_res=8):\n",
        "        super().__init__()\n",
        "        self.out_size = out_size\n",
        "        self.init_res = init_res\n",
        "        self.pos = SoftPositionEmbed(slot_dim)\n",
        "        self.deconv = nn.Sequential(\n",
        "            nn.ConvTranspose2d(slot_dim, hidden, 5, stride=2, padding=2, output_padding=1), nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(hidden, hidden, 5, stride=2, padding=2, output_padding=1), nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(hidden, hidden, 5, stride=2, padding=2, output_padding=1), nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(hidden, hidden, 5, stride=2, padding=2, output_padding=1), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(hidden, hidden, 5, padding=2), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(hidden, 4, 3, padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, slots):\n",
        "        B, K, D = slots.shape\n",
        "        x = slots.reshape(B*K, D, 1, 1).expand(B*K, D, self.init_res, self.init_res)\n",
        "        x = self.pos(x)\n",
        "        out = self.deconv(x)  # [B*K,4,H,W]\n",
        "        out = out.view(B, K, 4, self.out_size, self.out_size)\n",
        "        rgb = out[:, :, :3]\n",
        "        alpha = out[:, :, 3:4]\n",
        "        masks = F.softmax(alpha, dim=1)          # over slots\n",
        "        recon = (masks * rgb).sum(dim=1)\n",
        "        return recon, masks, rgb\n",
        "\n",
        "class SlotAttentionAE(nn.Module):\n",
        "    def __init__(self, num_slots=7, slot_dim=64, iters=3):\n",
        "        super().__init__()\n",
        "        self.encoder = CNNEncoder(in_ch=3, hidden=slot_dim)\n",
        "        self.slot_attn = SlotAttention(num_slots=num_slots, dim=slot_dim, iters=iters, hidden_dim=128)\n",
        "        self.decoder = SpatialBroadcastDecoder(slot_dim=slot_dim, hidden=64, out_size=IMG_SIZE, init_res=8)\n",
        "\n",
        "    def forward(self, x):\n",
        "        feats, _ = self.encoder(x)\n",
        "        slots, attn = self.slot_attn(feats)\n",
        "        recon, masks, rgb_slots = self.decoder(slots)\n",
        "        return recon, masks, rgb_slots, attn, slots"
      ],
      "metadata": {
        "id": "YI2kA76E3yUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 実験ディレクトリ作成"
      ],
      "metadata": {
        "id": "XpV3MU08SEpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 実験ディレクトリ作成（堅牢版：未定義変数があっても落ちない）=====\n",
        "import os, json, time\n",
        "\n",
        "RUN_NAME = \"slot_attention_moviA\"\n",
        "timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "OUT_DIR = f\"/content/results/{RUN_NAME}/{timestamp}\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "print(\"OUT_DIR =\", OUT_DIR)\n",
        "\n",
        "# ---- 未定義ならデフォルトを補完 ----\n",
        "if \"IMG_SIZE\" not in globals():\n",
        "    IMG_SIZE = 128\n",
        "if \"BATCH_SIZE\" not in globals():\n",
        "    BATCH_SIZE = 32\n",
        "\n",
        "# ここはキャッシュ方式だと未使用になりがちなので、無くてもOKにする\n",
        "if \"SHUFFLE_BUFFER\" not in globals():\n",
        "    SHUFFLE_BUFFER = None\n",
        "\n",
        "if \"video_key\" not in globals():\n",
        "    video_key = None\n",
        "if \"mask_key\" not in globals():\n",
        "    mask_key = None\n",
        "\n",
        "# 学習ハイパラ（未定義ならデフォルト）\n",
        "if \"LR\" not in globals():\n",
        "    LR = 4e-4\n",
        "if \"WARMUP_STEPS\" not in globals():\n",
        "    WARMUP_STEPS = 10_000\n",
        "if \"DECAY_STEPS\" not in globals():\n",
        "    DECAY_STEPS = 100_000\n",
        "if \"DECAY_RATE\" not in globals():\n",
        "    DECAY_RATE = 0.5\n",
        "\n",
        "# モデル設定（あなたの実装に合わせて記録）\n",
        "config = dict(\n",
        "    RUN_NAME=RUN_NAME,\n",
        "    timestamp=timestamp,\n",
        "    IMG_SIZE=IMG_SIZE,\n",
        "    BATCH_SIZE=BATCH_SIZE,\n",
        "    SHUFFLE_BUFFER=SHUFFLE_BUFFER,  # キャッシュ方式なら None でもOK\n",
        "    num_slots=7,\n",
        "    slot_dim=64,\n",
        "    iters=3,\n",
        "    lr=LR,\n",
        "    warmup_steps=WARMUP_STEPS,\n",
        "    decay_steps=DECAY_STEPS,\n",
        "    decay_rate=DECAY_RATE,\n",
        "    video_key=video_key,\n",
        "    mask_key=mask_key,\n",
        ")\n",
        "\n",
        "with open(os.path.join(OUT_DIR, \"config.json\"), \"w\") as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "print(\"saved config.json\")"
      ],
      "metadata": {
        "id": "AS0v4OZ16gO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 学習設定（LR warmup + exp decay）・モデル初期化"
      ],
      "metadata": {
        "id": "S-RKP88vSH90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_SLOTS = 7\n",
        "SLOT_DIM = 64\n",
        "ITERS = 3\n",
        "\n",
        "LR = 4e-4\n",
        "WARMUP_STEPS = 10_000\n",
        "DECAY_STEPS  = 100_000\n",
        "DECAY_RATE   = 0.5\n",
        "\n",
        "def lr_schedule(step):\n",
        "    if step < WARMUP_STEPS:\n",
        "        return LR * (step / max(1, WARMUP_STEPS))\n",
        "    decay = DECAY_RATE ** ((step - WARMUP_STEPS) / DECAY_STEPS)\n",
        "    return LR * decay\n",
        "\n",
        "def set_lr(opt, lr):\n",
        "    for pg in opt.param_groups:\n",
        "        pg[\"lr\"] = lr\n",
        "\n",
        "model = SlotAttentionAE(num_slots=NUM_SLOTS, slot_dim=SLOT_DIM, iters=ITERS).to(device)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=LR, betas=(0.9, 0.999), eps=1e-8)\n",
        "\n",
        "print(\"params(M):\", sum(p.numel() for p in model.parameters())/1e6)"
      ],
      "metadata": {
        "id": "MID12KaT6gYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 可視化（必須）— 固定バッチの input / recon / slot分解 を保存"
      ],
      "metadata": {
        "id": "gH6nX7M0SLli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def denorm(x):\n",
        "    return (x.clamp(-1, 1) + 1) / 2\n",
        "\n",
        "@torch.no_grad()\n",
        "def save_recon_figure(model, x_batch, out_path, max_slots_to_show=4):\n",
        "    model.eval()\n",
        "    x = x_batch.to(device)\n",
        "    recon, masks, rgb_slots, attn, slots = model(x)\n",
        "\n",
        "    x0 = denorm(x).cpu().numpy()\n",
        "    r0 = denorm(recon).cpu().numpy()\n",
        "    m0 = masks.cpu().numpy()\n",
        "    s0 = denorm(rgb_slots).cpu().numpy()\n",
        "\n",
        "    B, K = m0.shape[0], m0.shape[1]\n",
        "    showK = min(K, max_slots_to_show)\n",
        "\n",
        "    fig, axes = plt.subplots(B, 2+showK, figsize=(3*(2+showK), 3*B))\n",
        "    if B == 1:\n",
        "        axes = np.expand_dims(axes, 0)\n",
        "\n",
        "    for i in range(B):\n",
        "        axes[i,0].imshow(np.transpose(x0[i], (1,2,0)))\n",
        "        axes[i,0].set_title(\"input\"); axes[i,0].axis(\"off\")\n",
        "\n",
        "        axes[i,1].imshow(np.transpose(r0[i], (1,2,0)))\n",
        "        axes[i,1].set_title(\"recon\"); axes[i,1].axis(\"off\")\n",
        "\n",
        "        for j in range(showK):\n",
        "            img = s0[i,j] * m0[i,j]\n",
        "            axes[i,2+j].imshow(np.transpose(img, (1,2,0)))\n",
        "            axes[i,2+j].set_title(f\"slot{j}\"); axes[i,2+j].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    fig.savefig(out_path, dpi=150)\n",
        "    plt.close(fig)\n",
        "\n",
        "# 学習前の可視化を必ず保存（比較の基準）\n",
        "save_recon_figure(model, fixed_x, os.path.join(OUT_DIR, \"recon_step000000.png\"), max_slots_to_show=4)\n",
        "print(\"saved:\", os.path.join(OUT_DIR, \"recon_step000000.png\"))"
      ],
      "metadata": {
        "id": "mp5Sxf227RHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ARI（任意：maskがある場合のみ）"
      ],
      "metadata": {
        "id": "KBrvBgeaSOXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adjusted_rand_index(labels_true, labels_pred):\n",
        "    lt = labels_true.reshape(-1)\n",
        "    lp = labels_pred.reshape(-1)\n",
        "\n",
        "    _, lt = np.unique(lt, return_inverse=True)\n",
        "    _, lp = np.unique(lp, return_inverse=True)\n",
        "\n",
        "    n = lt.shape[0]\n",
        "    if n == 0:\n",
        "        return 0.0\n",
        "\n",
        "    ct = np.zeros((lt.max()+1, lp.max()+1), dtype=np.int64)\n",
        "    np.add.at(ct, (lt, lp), 1)\n",
        "\n",
        "    def comb2(x):\n",
        "        return x * (x - 1) // 2\n",
        "\n",
        "    sum_comb_c = comb2(ct).sum()\n",
        "    sum_comb_rows = comb2(ct.sum(axis=1)).sum()\n",
        "    sum_comb_cols = comb2(ct.sum(axis=0)).sum()\n",
        "    comb_n = comb2(n)\n",
        "\n",
        "    if comb_n == 0:\n",
        "        return 0.0\n",
        "\n",
        "    expected = sum_comb_rows * sum_comb_cols / comb_n\n",
        "    max_index = 0.5 * (sum_comb_rows + sum_comb_cols)\n",
        "    denom = (max_index - expected)\n",
        "    if denom == 0:\n",
        "        return 0.0\n",
        "    return float((sum_comb_c - expected) / denom)\n",
        "\n",
        "@torch.no_grad()\n",
        "def compute_ari_on_fixed_batch(model, x_batch, m_batch):\n",
        "    if m_batch is None:\n",
        "        return None\n",
        "    model.eval()\n",
        "    x = x_batch.to(device)\n",
        "    recon, masks, *_ = model(x)\n",
        "    pred = masks.squeeze(2).argmax(dim=1).cpu().numpy().astype(np.int64)  # [B,H,W]\n",
        "    gt = m_batch.cpu().numpy().astype(np.int64)\n",
        "    aris = [adjusted_rand_index(gt[i], pred[i]) for i in range(pred.shape[0])]\n",
        "    return float(np.mean(aris))\n",
        "\n",
        "print(\"initial ARI:\", compute_ari_on_fixed_batch(model, fixed_x, fixed_m))"
      ],
      "metadata": {
        "id": "93auadLA7Wnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 学習ループ（CSV保存＋曲線保存＋定期可視化保存）"
      ],
      "metadata": {
        "id": "gGFzZwGPSUxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== 学習セル（float16キャッシュ対応：入力をfloat32に統一） ======\n",
        "import os, csv\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 学習設定\n",
        "TOTAL_STEPS = 50000\n",
        "LOG_EVERY = 100\n",
        "VIS_EVERY = 2000\n",
        "VAL_EVERY = 2000\n",
        "\n",
        "metrics_path = os.path.join(OUT_DIR, \"metrics.csv\")\n",
        "with open(metrics_path, \"w\") as f:\n",
        "    f.write(\"step,loss,lr,val_loss,ari\\n\")\n",
        "\n",
        "def plot_loss_curve(csv_path, out_path):\n",
        "    steps, losses, vlosses = [], [], []\n",
        "    with open(csv_path, \"r\") as f:\n",
        "        r = csv.DictReader(f)\n",
        "        for row in r:\n",
        "            steps.append(int(row[\"step\"]))\n",
        "            losses.append(float(row[\"loss\"]))\n",
        "            vlosses.append(float(row[\"val_loss\"]) if row[\"val_loss\"] != \"\" else np.nan)\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot(steps, losses, label=\"train_loss\")\n",
        "    if np.isfinite(np.nanmin(vlosses)):\n",
        "        plt.plot(steps, vlosses, label=\"val_loss\")\n",
        "    plt.xlabel(\"step\"); plt.ylabel(\"MSE\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path, dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_mse(model, loader, num_batches=10):\n",
        "    if loader is None:\n",
        "        return None\n",
        "    model.eval()\n",
        "    it = iter(loader)\n",
        "    losses = []\n",
        "    for _ in range(num_batches):\n",
        "        try:\n",
        "            (x,) = next(it)\n",
        "        except StopIteration:\n",
        "            it = iter(loader)\n",
        "            (x,) = next(it)\n",
        "\n",
        "        x = x.to(device, non_blocking=True).float()  # ★重要：float32へ統一\n",
        "        recon, *_ = model(x)\n",
        "        losses.append(F.mse_loss(recon, x).item())\n",
        "    return float(np.mean(losses))\n",
        "\n",
        "model.train()\n",
        "train_it = iter(train_loader)\n",
        "pbar = tqdm(range(1, TOTAL_STEPS+1))\n",
        "\n",
        "for step in pbar:\n",
        "    try:\n",
        "        (x,) = next(train_it)\n",
        "    except StopIteration:\n",
        "        train_it = iter(train_loader)\n",
        "        (x,) = next(train_it)\n",
        "\n",
        "    x = x.to(device, non_blocking=True).float()  # ★重要：float32へ統一\n",
        "\n",
        "    lr = lr_schedule(step)\n",
        "    set_lr(opt, lr)\n",
        "\n",
        "    recon, masks, rgb_slots, attn, slots = model(x)\n",
        "    loss = F.mse_loss(recon, x)\n",
        "\n",
        "    opt.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "    if step % LOG_EVERY == 0:\n",
        "        pbar.set_description(f\"step {step} | loss {loss.item():.4f} | lr {lr:.2e}\")\n",
        "\n",
        "    val_loss = \"\"\n",
        "    ari = \"\"\n",
        "\n",
        "    if (step % VAL_EVERY == 0) or (step == 1):\n",
        "        val = eval_mse(model, val_loader, num_batches=10)\n",
        "        val_loss = \"\" if val is None else f\"{val:.6f}\"\n",
        "        # fixed_x もfloat32へ\n",
        "        ari_val = compute_ari_on_fixed_batch(model, fixed_x.float(), fixed_m)\n",
        "        ari = \"\" if ari_val is None else f\"{ari_val:.6f}\"\n",
        "\n",
        "    if step % VIS_EVERY == 0:\n",
        "        save_recon_figure(model, fixed_x.float(), os.path.join(OUT_DIR, f\"recon_step{step:06d}.png\"), max_slots_to_show=4)\n",
        "        plot_loss_curve(metrics_path, os.path.join(OUT_DIR, \"loss_curve.png\"))\n",
        "\n",
        "    if (step % LOG_EVERY == 0) or (step == 1) or (step % VAL_EVERY == 0):\n",
        "        with open(metrics_path, \"a\") as f:\n",
        "            f.write(f\"{step},{loss.item():.6f},{lr:.8f},{val_loss},{ari}\\n\")\n",
        "\n",
        "plot_loss_curve(metrics_path, os.path.join(OUT_DIR, \"loss_curve.png\"))\n",
        "print(\"saved metrics:\", metrics_path)\n",
        "print(\"saved loss curve:\", os.path.join(OUT_DIR, \"loss_curve.png\"))"
      ],
      "metadata": {
        "id": "J7Pu5FTt7aic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 結果をノートブック上で表示"
      ],
      "metadata": {
        "id": "sqiqZu9KSasw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import glob\n",
        "\n",
        "print(\"OUT_DIR:\", OUT_DIR)\n",
        "\n",
        "# 損失曲線\n",
        "curve = os.path.join(OUT_DIR, \"loss_curve.png\")\n",
        "if os.path.exists(curve):\n",
        "    display(Image.open(curve))\n",
        "\n",
        "# 最新 recon\n",
        "recon_imgs = sorted(glob.glob(os.path.join(OUT_DIR, \"recon_step*.png\")))\n",
        "print(\"num recon imgs:\", len(recon_imgs))\n",
        "if len(recon_imgs) > 0:\n",
        "    display(Image.open(recon_imgs[-1]))\n",
        "else:\n",
        "    # 途中保存前なら初期図だけあるはず\n",
        "    init_img = os.path.join(OUT_DIR, \"recon_step000000.png\")\n",
        "    if os.path.exists(init_img):\n",
        "        display(Image.open(init_img))"
      ],
      "metadata": {
        "id": "N2mZraY6GCSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 評価用パッケージ（Hungarian割当）"
      ],
      "metadata": {
        "id": "pqvFyDq3Scce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U scipy\n",
        "print(\"done\")"
      ],
      "metadata": {
        "id": "hKaSac5zSjk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 動画シーケンス DataLoader（TFDS → 連続Tフレーム）"
      ],
      "metadata": {
        "id": "7ynjLxH5__-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import IterableDataset, DataLoader\n",
        "\n",
        "class TFDSVideoSeqIterable(IterableDataset):\n",
        "    \"\"\"\n",
        "    yields:\n",
        "      x:  [T,3,H,W] float32 in [-1,1]\n",
        "      gt: [T,H,W]   int64 instance-id map (if mask_key exists) else None\n",
        "    \"\"\"\n",
        "    def __init__(self, tfds_dataset, video_key, mask_key=None, T=24, img_size=128,\n",
        "                 start_mode=\"random\", seed=0):\n",
        "        super().__init__()\n",
        "        self.ds = tfds_dataset\n",
        "        self.video_key = video_key\n",
        "        self.mask_key = mask_key\n",
        "        self.T = T\n",
        "        self.img_size = img_size\n",
        "        self.start_mode = start_mode\n",
        "        self.rng = np.random.RandomState(seed)\n",
        "\n",
        "    def __iter__(self):\n",
        "        for ex in tfds.as_numpy(self.ds):\n",
        "            vid = ex[self.video_key]  # [Tv,H,W,3] uint8\n",
        "            Tv = vid.shape[0]\n",
        "            if Tv < self.T:\n",
        "                continue\n",
        "\n",
        "            if self.start_mode == \"random\":\n",
        "                s = self.rng.randint(0, Tv - self.T + 1)\n",
        "            else:\n",
        "                s = 0\n",
        "\n",
        "            clip = vid[s:s+self.T].astype(np.float32) / 255.0\n",
        "            clip = clip * 2.0 - 1.0  # [-1,1]\n",
        "            x = torch.from_numpy(clip).permute(0,3,1,2)  # [T,3,H,W]\n",
        "            x = F.interpolate(x, size=(self.img_size, self.img_size), mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "            if self.mask_key is not None and self.mask_key in ex:\n",
        "                gt = ex[self.mask_key][s:s+self.T]\n",
        "                if gt.ndim == 4 and gt.shape[-1] == 1:\n",
        "                    gt = gt[...,0]\n",
        "                gt = torch.from_numpy(gt.astype(np.int64))  # [T,H,W]\n",
        "                gt = gt.unsqueeze(1).float()  # [T,1,H,W]\n",
        "                gt = F.interpolate(gt, size=(self.img_size, self.img_size), mode=\"nearest\").long()\n",
        "                gt = gt.squeeze(1)  # [T,H,W]\n",
        "                yield x.contiguous(), gt.contiguous()\n",
        "            else:\n",
        "                yield x.contiguous(), None\n",
        "\n",
        "def make_video_loader(tfds_ds, batch_size=1, T=24, img_size=128, n_videos=64, seed=0):\n",
        "    # まず評価は小さめに（安定＆速い）\n",
        "    ds = tfds_ds.take(n_videos)\n",
        "    it = TFDSVideoSeqIterable(ds, video_key=video_key, mask_key=mask_key, T=T, img_size=img_size,\n",
        "                             start_mode=\"random\", seed=seed)\n",
        "    return DataLoader(it, batch_size=batch_size, num_workers=0, pin_memory=True)\n",
        "\n",
        "EVAL_T = 24\n",
        "EVAL_VIDEOS = 64\n",
        "video_loader = make_video_loader(val_tfds if val_tfds is not None else train_tfds,\n",
        "                                 batch_size=1, T=EVAL_T, img_size=IMG_SIZE, n_videos=EVAL_VIDEOS, seed=123)\n",
        "print(\"video_loader ready\")"
      ],
      "metadata": {
        "id": "7U4F6sgZSlfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Slot Attention を動画に適用（フレームごと）"
      ],
      "metadata": {
        "id": "tbGBNz9sAD7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "@torch.no_grad()\n",
        "def run_model_on_clip(model, x_clip):\n",
        "    \"\"\"\n",
        "    x_clip: [T,3,H,W] (torch, [-1,1])\n",
        "    returns:\n",
        "      masks: [T,K,H,W] float\n",
        "      slots: [T,K,D]   float\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    T = x_clip.shape[0]\n",
        "    all_masks = []\n",
        "    all_slots = []\n",
        "    for t in range(T):\n",
        "        x = x_clip[t:t+1].to(device, non_blocking=True)  # [1,3,H,W]\n",
        "        recon, masks, rgb_slots, attn, slots = model(x)\n",
        "        # masks: [1,K,1,H,W] -> [K,H,W]\n",
        "        mt = masks[0,:,0].detach().cpu()\n",
        "        st = slots[0].detach().cpu()\n",
        "        all_masks.append(mt)\n",
        "        all_slots.append(st)\n",
        "    return torch.stack(all_masks, dim=0), torch.stack(all_slots, dim=0)"
      ],
      "metadata": {
        "id": "PowBdpwESr2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 追跡（HungarianでスロットIDを時間で揃える）"
      ],
      "metadata": {
        "id": "HG-hwQDpAHsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "def soft_iou(a, b, eps=1e-8):\n",
        "    # a,b: [H,W] float\n",
        "    inter = torch.minimum(a, b).sum()\n",
        "    union = torch.maximum(a, b).sum() + eps\n",
        "    return (inter / union).item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def track_slots_by_iou(masks_TKHW):\n",
        "    \"\"\"\n",
        "    masks_TKHW: [T,K,H,W]\n",
        "    returns:\n",
        "      perm_TK: list of permutations per t (t>=1) mapping current slots -> tracked IDs\n",
        "      tracked_masks: [T,K,H,W] aligned over time\n",
        "    \"\"\"\n",
        "    T, K, H, W = masks_TKHW.shape\n",
        "    tracked = [masks_TKHW[0]]  # [K,H,W]\n",
        "    prev = masks_TKHW[0]\n",
        "\n",
        "    perms = [np.arange(K)]\n",
        "    for t in range(1, T):\n",
        "        cur = masks_TKHW[t]\n",
        "\n",
        "        # cost[i,j] = 1 - IoU(prev_i, cur_j)\n",
        "        cost = np.zeros((K, K), dtype=np.float32)\n",
        "        for i in range(K):\n",
        "            for j in range(K):\n",
        "                cost[i, j] = 1.0 - soft_iou(prev[i], cur[j])\n",
        "\n",
        "        row, col = linear_sum_assignment(cost)  # row: prev idx, col: cur idx\n",
        "        # We want cur aligned to prev order (tracked IDs)\n",
        "        aligned = cur[col]  # [K,H,W] (tracked_id i corresponds to col[i])\n",
        "        perms.append(col.copy())\n",
        "\n",
        "        tracked.append(aligned)\n",
        "        prev = aligned\n",
        "\n",
        "    tracked_masks = torch.stack(tracked, dim=0)  # [T,K,H,W]\n",
        "    return perms, tracked_masks"
      ],
      "metadata": {
        "id": "u0icwdouSr_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GTありの場合：ID swap率（物体→スロット割当の入れ替わり）"
      ],
      "metadata": {
        "id": "vrPyJIRhALFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dominant_slot_for_object(gt_hw, tracked_masks_KHW, obj_id, min_pixels=50):\n",
        "    \"\"\"\n",
        "    gt_hw: [H,W] int\n",
        "    tracked_masks_KHW: [K,H,W] float\n",
        "    return slot_id or None\n",
        "    \"\"\"\n",
        "    obj = (gt_hw == obj_id)\n",
        "    if obj.sum().item() < min_pixels:\n",
        "        return None\n",
        "    # overlap score = sum(mask * obj)\n",
        "    scores = (tracked_masks_KHW * obj.float().unsqueeze(0)).sum(dim=(1,2))  # [K]\n",
        "    return int(scores.argmax().item())\n",
        "\n",
        "@torch.no_grad()\n",
        "def id_swap_rate_for_clip(gt_THW, tracked_masks_TKHW, background_id=0, min_pixels=50):\n",
        "    \"\"\"\n",
        "    gt_THW: [T,H,W] int\n",
        "    tracked_masks_TKHW: [T,K,H,W] float (tracked)\n",
        "    returns dict with swap_rate and details\n",
        "    \"\"\"\n",
        "    T, K, H, W = tracked_masks_TKHW.shape\n",
        "    swaps = 0\n",
        "    transitions = 0\n",
        "    per_obj = {}\n",
        "\n",
        "    # 対象物体ID（最初のフレームから抽出でも良いが、全フレームで抽出）\n",
        "    obj_ids = set()\n",
        "    for t in range(T):\n",
        "        ids = torch.unique(gt_THW[t]).tolist()\n",
        "        for oid in ids:\n",
        "            if oid != background_id:\n",
        "                obj_ids.add(int(oid))\n",
        "    obj_ids = sorted(list(obj_ids))\n",
        "\n",
        "    for oid in obj_ids:\n",
        "        last = None\n",
        "        obj_swaps = 0\n",
        "        obj_trans = 0\n",
        "        for t in range(T):\n",
        "            slot = dominant_slot_for_object(gt_THW[t], tracked_masks_TKHW[t], oid, min_pixels=min_pixels)\n",
        "            if slot is None:\n",
        "                continue\n",
        "            if last is not None:\n",
        "                obj_trans += 1\n",
        "                if slot != last:\n",
        "                    obj_swaps += 1\n",
        "            last = slot\n",
        "        if obj_trans > 0:\n",
        "            per_obj[oid] = {\"swaps\": obj_swaps, \"transitions\": obj_trans, \"swap_rate\": obj_swaps/obj_trans}\n",
        "            swaps += obj_swaps\n",
        "            transitions += obj_trans\n",
        "\n",
        "    swap_rate = (swaps / transitions) if transitions > 0 else None\n",
        "    return {\"swap_rate\": swap_rate, \"swaps\": swaps, \"transitions\": transitions, \"per_obj\": per_obj}"
      ],
      "metadata": {
        "id": "XBqrjddeSsHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 自己一貫性（追跡slotのIoU維持）"
      ],
      "metadata": {
        "id": "-289k1r5APVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def self_consistency_iou(tracked_masks_TKHW):\n",
        "    \"\"\"\n",
        "    連続フレームの同一tracked slot間IoUの平均\n",
        "    \"\"\"\n",
        "    T, K, H, W = tracked_masks_TKHW.shape\n",
        "    vals = []\n",
        "    for t in range(1, T):\n",
        "        for k in range(K):\n",
        "            vals.append(soft_iou(tracked_masks_TKHW[t-1,k], tracked_masks_TKHW[t,k]))\n",
        "    return float(np.mean(vals)) if len(vals) else None"
      ],
      "metadata": {
        "id": "S84TW_EtSsPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 評価ループ（動画N本）→ 平均ID swap率 / 一貫性 を出力"
      ],
      "metadata": {
        "id": "PY3J9a9oAUYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "results = []\n",
        "swap_rates = []\n",
        "consistencies = []\n",
        "\n",
        "for i, batch in enumerate(video_loader):\n",
        "    x_clip, gt = batch\n",
        "    x_clip = x_clip[0]            # [T,3,H,W]\n",
        "    gt = gt[0] if gt is not None else None  # [T,H,W] or None\n",
        "\n",
        "    masks_TKHW, slots_TKD = run_model_on_clip(model, x_clip)\n",
        "    _, tracked_masks = track_slots_by_iou(masks_TKHW)\n",
        "\n",
        "    cons = self_consistency_iou(tracked_masks)\n",
        "    consistencies.append(cons)\n",
        "\n",
        "    if gt is not None:\n",
        "        out = id_swap_rate_for_clip(gt, tracked_masks, background_id=0, min_pixels=50)\n",
        "        swap_rates.append(out[\"swap_rate\"] if out[\"swap_rate\"] is not None else np.nan)\n",
        "        results.append({\"video\": i, \"swap_rate\": out[\"swap_rate\"], \"consistency_iou\": cons,\n",
        "                        \"swaps\": out[\"swaps\"], \"transitions\": out[\"transitions\"]})\n",
        "    else:\n",
        "        results.append({\"video\": i, \"swap_rate\": None, \"consistency_iou\": cons})\n",
        "\n",
        "print(\"N videos:\", len(results))\n",
        "print(\"mean self-consistency IoU:\", float(np.nanmean(consistencies)))\n",
        "\n",
        "if len(swap_rates) > 0:\n",
        "    print(\"mean ID swap rate:\", float(np.nanmean(swap_rates)))"
      ],
      "metadata": {
        "id": "R-wLWNglSsXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 可視化（追跡スロットの推移を動画っぽく見る）"
      ],
      "metadata": {
        "id": "N5rBbHTmAYJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "@torch.no_grad()\n",
        "def show_tracking_example(model, tfds_source, T=16, max_slots=4, seed=0):\n",
        "    # 1 videoだけ取る\n",
        "    it = TFDSVideoSeqIterable(tfds_source.take(1), video_key=video_key, mask_key=mask_key,\n",
        "                             T=T, img_size=IMG_SIZE, start_mode=\"random\", seed=seed)\n",
        "    x_clip, gt = next(iter(it))\n",
        "    masks_TKHW, _ = run_model_on_clip(model, x_clip)\n",
        "    _, tracked = track_slots_by_iou(masks_TKHW)\n",
        "\n",
        "    # display\n",
        "    def denorm(x): return (x.clamp(-1,1)+1)/2\n",
        "    x_np = denorm(x_clip).permute(0,2,3,1).cpu().numpy()  # [T,H,W,3]\n",
        "\n",
        "    showK = min(tracked.shape[1], max_slots)\n",
        "    fig, axes = plt.subplots(showK+1, T, figsize=(1.6*T, 1.6*(showK+1)))\n",
        "\n",
        "    for t in range(T):\n",
        "        axes[0, t].imshow(x_np[t])\n",
        "        axes[0, t].axis(\"off\")\n",
        "        if t == 0: axes[0, t].set_title(\"input\")\n",
        "\n",
        "    for k in range(showK):\n",
        "        for t in range(T):\n",
        "            axes[1+k, t].imshow(tracked[t, k].cpu().numpy(), vmin=0, vmax=1)\n",
        "            axes[1+k, t].axis(\"off\")\n",
        "            if t == 0: axes[1+k, t].set_title(f\"slot{k}\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "show_tracking_example(model, val_tfds if val_tfds is not None else train_tfds, T=16, max_slots=4, seed=0)"
      ],
      "metadata": {
        "id": "s8z_0ETuS54D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 結果 results を DataFrame 化して保存（CSV）"
      ],
      "metadata": {
        "id": "KpbIp3wcAcGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# results: 既に作成済み（list of dict）を想定\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "# swap_rate が None の場合は NaN\n",
        "if \"swap_rate\" in df.columns:\n",
        "    df[\"swap_rate\"] = pd.to_numeric(df[\"swap_rate\"], errors=\"coerce\")\n",
        "if \"consistency_iou\" in df.columns:\n",
        "    df[\"consistency_iou\"] = pd.to_numeric(df[\"consistency_iou\"], errors=\"coerce\")\n",
        "\n",
        "display(df.head())\n",
        "\n",
        "# 保存先（OUT_DIR がある想定。無ければ /content/results_eval を作る）\n",
        "EVAL_OUT = OUT_DIR if \"OUT_DIR\" in globals() else \"/content/results_eval\"\n",
        "os.makedirs(EVAL_OUT, exist_ok=True)\n",
        "\n",
        "csv_path = os.path.join(EVAL_OUT, \"video_eval_summary.csv\")\n",
        "df.to_csv(csv_path, index=False)\n",
        "print(\"saved:\", csv_path)"
      ],
      "metadata": {
        "id": "saSGpnglUDsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 全体サマリの可視化（ヒストグラム＋散布図）"
      ],
      "metadata": {
        "id": "OWKX_rOgAgD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def save_fig(path):\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(path, dpi=150)\n",
        "    plt.close()\n",
        "    print(\"saved:\", path)\n",
        "\n",
        "# 1) swap_rate ヒストグラム（GTありのとき）\n",
        "if \"swap_rate\" in df.columns and df[\"swap_rate\"].notna().any():\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.hist(df[\"swap_rate\"].dropna().values, bins=20)\n",
        "    plt.xlabel(\"ID swap rate (lower is better)\")\n",
        "    plt.ylabel(\"count\")\n",
        "    plt.title(\"Distribution of ID swap rate\")\n",
        "    save_fig(os.path.join(EVAL_OUT, \"hist_swap_rate.png\"))\n",
        "else:\n",
        "    print(\"swap_rate is not available (GT mask_key is None or missing).\")\n",
        "\n",
        "# 2) consistency IoU ヒストグラム（GT無しでも可）\n",
        "if \"consistency_iou\" in df.columns and df[\"consistency_iou\"].notna().any():\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.hist(df[\"consistency_iou\"].dropna().values, bins=20)\n",
        "    plt.xlabel(\"Self-consistency IoU (higher is better)\")\n",
        "    plt.ylabel(\"count\")\n",
        "    plt.title(\"Distribution of self-consistency IoU\")\n",
        "    save_fig(os.path.join(EVAL_OUT, \"hist_consistency_iou.png\"))\n",
        "\n",
        "# 3) swap_rate vs consistency_iou 散布図（両方ある場合）\n",
        "if (\"swap_rate\" in df.columns and df[\"swap_rate\"].notna().any()\n",
        "    and \"consistency_iou\" in df.columns and df[\"consistency_iou\"].notna().any()):\n",
        "    plt.figure(figsize=(6,5))\n",
        "    d2 = df.dropna(subset=[\"swap_rate\", \"consistency_iou\"])\n",
        "    plt.scatter(d2[\"swap_rate\"].values, d2[\"consistency_iou\"].values, s=25)\n",
        "    plt.xlabel(\"ID swap rate (↓)\")\n",
        "    plt.ylabel(\"Self-consistency IoU (↑)\")\n",
        "    plt.title(\"Swap rate vs Consistency\")\n",
        "    save_fig(os.path.join(EVAL_OUT, \"scatter_swap_vs_consistency.png\"))"
      ],
      "metadata": {
        "id": "sGd5m8YCUE89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 動画ごとの折れ線（時系列ではなく “動画index” で並べて比較）"
      ],
      "metadata": {
        "id": "T7gLziSVAjxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# swap_rate の動画別プロット\n",
        "if \"swap_rate\" in df.columns and df[\"swap_rate\"].notna().any():\n",
        "    plt.figure(figsize=(10,3))\n",
        "    plt.plot(df[\"video\"].values, df[\"swap_rate\"].values, marker=\"o\", linewidth=1)\n",
        "    plt.xlabel(\"video index\")\n",
        "    plt.ylabel(\"ID swap rate\")\n",
        "    plt.title(\"ID swap rate per video\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    path = os.path.join(EVAL_OUT, \"line_swap_rate_per_video.png\")\n",
        "    plt.savefig(path, dpi=150)\n",
        "    plt.close()\n",
        "    print(\"saved:\", path)\n",
        "\n",
        "# consistency_iou の動画別プロット\n",
        "if \"consistency_iou\" in df.columns and df[\"consistency_iou\"].notna().any():\n",
        "    plt.figure(figsize=(10,3))\n",
        "    plt.plot(df[\"video\"].values, df[\"consistency_iou\"].values, marker=\"o\", linewidth=1)\n",
        "    plt.xlabel(\"video index\")\n",
        "    plt.ylabel(\"self-consistency IoU\")\n",
        "    plt.title(\"Self-consistency IoU per video\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    path = os.path.join(EVAL_OUT, \"line_consistency_per_video.png\")\n",
        "    plt.savefig(path, dpi=150)\n",
        "    plt.close()\n",
        "    print(\"saved:\", path)"
      ],
      "metadata": {
        "id": "cJc0P9X2UHvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1本の動画をGIF保存（入力＋tracked masks）"
      ],
      "metadata": {
        "id": "6W9oNWTmAmvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U imageio\n",
        "print(\"done\")"
      ],
      "metadata": {
        "id": "F_XZnh0yUQFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== GIF生成に必要な関数を全部まとめて再定義（順番実行で落ちない） =====\n",
        "!pip -q install -U imageio\n",
        "import os\n",
        "import numpy as np\n",
        "import imageio.v2 as imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "# 必須チェック\n",
        "assert \"model\" in globals(), \"model が未定義です（学習済みモデルを用意してください）\"\n",
        "assert \"train_tfds\" in globals(), \"train_tfds が未定義です（データロードセルを先に実行）\"\n",
        "assert \"video_key\" in globals(), \"video_key が未定義です\"\n",
        "if \"val_tfds\" not in globals():\n",
        "    val_tfds = None\n",
        "if \"IMG_SIZE\" not in globals():\n",
        "    IMG_SIZE = 128\n",
        "if \"device\" not in globals():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# もし mask_key が未定義でもOK\n",
        "if \"mask_key\" not in globals():\n",
        "    mask_key = None\n",
        "\n",
        "# もし EVAL_OUT が未定義なら作る\n",
        "if \"EVAL_OUT\" not in globals():\n",
        "    EVAL_OUT = \"/content/results_eval\"\n",
        "    os.makedirs(EVAL_OUT, exist_ok=True)\n",
        "\n",
        "# --- クリップ取り出し（TFDS） ---\n",
        "import tensorflow_datasets as tfds\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TFDSVideoSeqIterable:\n",
        "    def __init__(self, tfds_dataset, video_key, mask_key=None, T=16, img_size=128, seed=0):\n",
        "        self.ds = tfds_dataset\n",
        "        self.video_key = video_key\n",
        "        self.mask_key = mask_key\n",
        "        self.T = T\n",
        "        self.img_size = img_size\n",
        "        self.rng = np.random.RandomState(seed)\n",
        "\n",
        "    def __iter__(self):\n",
        "        for ex in tfds.as_numpy(self.ds):\n",
        "            vid = ex[self.video_key]  # [Tv,H,W,3]\n",
        "            Tv = vid.shape[0]\n",
        "            if Tv < self.T:\n",
        "                continue\n",
        "            s = self.rng.randint(0, Tv - self.T + 1)\n",
        "            clip = vid[s:s+self.T].astype(np.float32)/255.0\n",
        "            clip = clip*2.0 - 1.0\n",
        "            x = torch.from_numpy(clip).permute(0,3,1,2)  # [T,3,H,W]\n",
        "            x = F.interpolate(x, size=(self.img_size,self.img_size), mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "            if self.mask_key is not None and self.mask_key in ex:\n",
        "                gt = ex[self.mask_key][s:s+self.T]\n",
        "                if gt.ndim == 4 and gt.shape[-1] == 1:\n",
        "                    gt = gt[...,0]\n",
        "                gt = torch.from_numpy(gt.astype(np.int64))\n",
        "                gt = gt.unsqueeze(1).float()\n",
        "                gt = F.interpolate(gt, size=(self.img_size,self.img_size), mode=\"nearest\").long().squeeze(1)\n",
        "                yield x.contiguous(), gt.contiguous()\n",
        "            else:\n",
        "                yield x.contiguous(), None\n",
        "\n",
        "@torch.no_grad()\n",
        "def run_model_on_clip(model, x_clip):\n",
        "    model.eval()\n",
        "    all_masks = []\n",
        "    for t in range(x_clip.shape[0]):\n",
        "        x = x_clip[t:t+1].to(device, non_blocking=True).float()\n",
        "        recon, masks, rgb_slots, attn, slots = model(x)\n",
        "        # masks: [1,K,1,H,W] -> [K,H,W]\n",
        "        all_masks.append(masks[0,:,0].detach().cpu())\n",
        "    return torch.stack(all_masks, dim=0)  # [T,K,H,W]\n",
        "\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "def soft_iou(a, b, eps=1e-8):\n",
        "    inter = torch.minimum(a, b).sum()\n",
        "    union = torch.maximum(a, b).sum() + eps\n",
        "    return (inter/union).item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def track_slots_by_iou(masks_TKHW):\n",
        "    T, K, H, W = masks_TKHW.shape\n",
        "    tracked = [masks_TKHW[0]]\n",
        "    prev = masks_TKHW[0]\n",
        "    for t in range(1, T):\n",
        "        cur = masks_TKHW[t]\n",
        "        cost = np.zeros((K,K), dtype=np.float32)\n",
        "        for i in range(K):\n",
        "            for j in range(K):\n",
        "                cost[i,j] = 1.0 - soft_iou(prev[i], cur[j])\n",
        "        row, col = linear_sum_assignment(cost)\n",
        "        aligned = cur[col]\n",
        "        tracked.append(aligned)\n",
        "        prev = aligned\n",
        "    return torch.stack(tracked, dim=0)\n",
        "\n",
        "def denorm01(x):\n",
        "    return (x.clamp(-1,1) + 1) / 2\n",
        "\n",
        "def fig_to_uint8_rgb(fig):\n",
        "    fig.canvas.draw()\n",
        "    buf = np.asarray(fig.canvas.buffer_rgba())  # (H,W,4)\n",
        "    return buf[..., :3].copy()\n",
        "\n",
        "@torch.no_grad()\n",
        "def render_clip_frames(x_clip, tracked_masks, max_slots=4):\n",
        "    T = x_clip.shape[0]\n",
        "    K = tracked_masks.shape[1]\n",
        "    showK = min(K, max_slots)\n",
        "\n",
        "    x_np = denorm01(x_clip).permute(0,2,3,1).cpu().numpy()\n",
        "\n",
        "    frames = []\n",
        "    for t in range(T):\n",
        "        fig, axes = plt.subplots(1, 1+showK, figsize=(3*(1+showK), 3))\n",
        "        axes[0].imshow(x_np[t]); axes[0].set_title(f\"input t={t}\"); axes[0].axis(\"off\")\n",
        "        for k in range(showK):\n",
        "            mk = tracked_masks[t,k].cpu().numpy()\n",
        "            axes[1+k].imshow(mk, vmin=0, vmax=1)\n",
        "            axes[1+k].set_title(f\"slot{k}\")\n",
        "            axes[1+k].axis(\"off\")\n",
        "        plt.tight_layout()\n",
        "        frames.append(fig_to_uint8_rgb(fig))\n",
        "        plt.close(fig)\n",
        "    return frames\n",
        "\n",
        "@torch.no_grad()\n",
        "def save_tracking_gif(model, tfds_source, out_gif, T=16, max_slots=4, seed=0, fps=6):\n",
        "    it = TFDSVideoSeqIterable(tfds_source.take(1), video_key=video_key, mask_key=mask_key,\n",
        "                             T=T, img_size=IMG_SIZE, seed=seed)\n",
        "    x_clip, gt = next(iter(it))\n",
        "    masks_TKHW = run_model_on_clip(model, x_clip)\n",
        "    tracked = track_slots_by_iou(masks_TKHW)\n",
        "    frames = render_clip_frames(x_clip, tracked, max_slots=max_slots)\n",
        "    imageio.mimsave(out_gif, frames, fps=fps)\n",
        "    print(\"saved gif:\", out_gif)\n",
        "\n",
        "print(\"save_tracking_gif is ready ✅  EVAL_OUT=\", EVAL_OUT)"
      ],
      "metadata": {
        "id": "siK69pwoCX5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gif_path = os.path.join(EVAL_OUT, \"tracking_example.gif\")\n",
        "save_tracking_gif(model, val_tfds if val_tfds is not None else train_tfds,\n",
        "                  gif_path, T=16, max_slots=4, seed=0, fps=6)"
      ],
      "metadata": {
        "id": "BvVuoq60VZIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "display(Image(filename=gif_path))"
      ],
      "metadata": {
        "id": "zDV_Gz8HVepe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob, os\n",
        "\n",
        "files = sorted(glob.glob(os.path.join(EVAL_OUT, \"*\")))\n",
        "print(\"Saved files:\")\n",
        "for f in files:\n",
        "    print(\" -\", f)"
      ],
      "metadata": {
        "id": "mRrBCP0DUoGH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}